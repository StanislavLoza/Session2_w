---
title: "MiM Workshop Session 2: Inferential Statistics"
author: "Your name goes here"
date: "DATE HERE" 
output:
  html_document:
    theme: flatly
    highlight: zenburn
    toc: yes
    toc_float: yes
---

```{r, setup, include=FALSE}
library(plotly)
library(lubridate)
library(janitor)   # examine and clean dirty data. Will use function: clean_names()
library(scales)    # to use scale_x_date(labels = date_format("%Y-%b"))
library(tidytext)

library(mosaic)   # Load additional packages here 
library(tidyverse)
library(infer)
library(here)
library(gapminder)
library(tidyquant)
library(viridis) # install if necessary
options(digits = 3)

# Some customization. You can alter or delete as desired (if you know what you are doing).
knitr::opts_chunk$set(
  tidy = FALSE,     # display code as typed
  size = "small")   # slightly smaller font for code
```

In preparing for this workshop, you may find useful the material from the **Analytics with R** website on 

- [Visualising data](http://telapps.london.edu/analytics_with_R/visualise_data.html) 
- [Reshaping data](http://telapps.london.edu/analytics_with_R/reshape_data.html)


# Task 1: Plane Crashes

Let us look at some data about airplane crashes (from [Kaggle](https://www.kaggle.com/saurograndi/airplane-crashes-since-1908/)).
This dataset lists all plane crashes between 1908 and 2009. 
Because this is real world data, it's messy, so we'll create a few extra variables and get rid of incomplete rows. 
I am providing the code to clean the data in the cell below, so you don't have to worry about it.

```{r clean-plane-data, message=FALSE, warning=FALSE}
library(lubridate)  # Loads functions like mdy(), year(), and month() that make handling of dates easy

crashes_raw <- read_csv(here::here("Data", "Airplane_Crashes_and_Fatalities_Since_1908.csv"))

crashes <- crashes_raw %>% 
  # Create new variables related to dates
  mutate(Date = mdy(Date),
         Year = year(Date),
         Month = month(Date, label = TRUE)) %>% 
  
  # Get rid of rows with missing data or where there were no fatalities
  filter(!is.na(Fatalities),
         !is.na(Aboard),
         !is.na(Operator),
         Fatalities > 0,) %>% 
  
  # Make a new variable that indicates if the flight was operated by the military
  mutate(Military = ifelse(str_detect(Operator, "Military"), "Military", "Not military")) %>% 
  
  # Make a new variable that shows what percentage of people on the flight died
  mutate(percent_passengers_died = Fatalities / Aboard)
```


## CRASHES PER YEAR:

Summarise the data to obtain the total number of crashes for every year. 
Fit a polynomial smoothing method to the data. 

```{r Crashes per year}
crashes_per_year <- crashes %>% 
  group_by(Year) %>% 
  summarise(count = n())

# ggplot:







```



## TOTAL DEATHS BY AIRLINE

1. Create a tibble containing total number of fatalities for non-military flights, ordered by the airline with the largest total number of fatalities first. Print out the resulting tibble.

```{r 1 Fatalities}







```


2. Present this information (only for airlines those total fatalities are greater or equal to 500) using the chart of your choice which you believe would best represent the data. Ensure your axes & graph are labelled clearly.

```{r 2 Fatalities}












```



## PERCENTAGE OF DEATHS IN CRASHES

Obtain a plot that shows the percentage of passengers that died in crashes over time, coloured by whether or not it was a military flight. 
- Standard Plot: use scatter plot smoothed with a polynomial fit
- Advanced Plot: first calculate the average crashes per year for each group: whether it was a military or non-military plane. Next plot the smoothed scatter plot coloured by (non)military information.

```{r percent-died}
# --- Option 1: plot standard








# --- Option 2: plot on average









```


# Task 2: Climate change and temperature anomalies 

In the last workshop, you have worked with monthly weather anomalies. 
However, we might be interested in average annual anomalies. 
We can do this by using `group_by()` and `summarise()`, followed by a scatter plot to display the result. 
Find the average annual anomalies (grouping by year) and plot these using a smoothing method. 

```{r averaging, warning=FALSE}
weather <- read_csv("https://data.giss.nasa.gov/gistemp/tabledata_v3/NH.Ts+dSST.csv", 
           skip = 1,   # number of lines to skip before reading data
           na = "***") # NAs in csv are coded with ***

tidyweather <- weather %>% 
  select(Year:Dec) %>% 
  pivot_longer(Jan:Dec, names_to = "Month", values_to = "delta")

#creating yearly averages





#plotting the data:






```


# Task 3: Confidence Intervals

[NASA points out on their website](https://earthobservatory.nasa.gov/world-of-change/decadaltemp.php) that: 

A one-degree global change is significant because it takes a vast amount of heat to warm all the oceans, atmosphere, and land by that much. In the past, a one- to two-degree drop was all it took to plunge the Earth into the Little Ice Age.

## CONFIDENCE INTERVAL WITH FORMULA

We are going to work with the average temperature deviation (delta) since 2011.
Construct a **90% confidence** interval for the average annual delta since 2011. 
Recall that the tibble `comparison` has already grouped temperature anomalies according to time intervals; we are only interested in what is happening between 2011-present.

Hints: 

- this tibble contains missing values, however summarise() verb expects a tibble without missing values. You could use na.omit() function before using summarise() in order to omit values that are not available.
- obtain the number of observations before deciding if you are allowed to use the z-value or not (i.e. does CLT hold?)
- if CLT holds, then proceed to calculate the confidence interval

```{r, formula_CI}
comparison <- tidyweather %>% 
  filter(Year >= 1881) %>% # remove years prior to 1881
  # create new variable 'interval', and assign values based on criteria below
  mutate(interval = 
    case_when(
    Year %in% c(1881:1920) ~ "1881-1920",
    Year %in% c(1921:1950) ~ "1921-1950",
    Year %in% c(1951:1980) ~ "1951-1980",
    Year %in% c(1981:2010) ~ "1981-2010",
    TRUE ~ "2011-present"))

# extract data just for 2011 to present
data_2011_to_present <- comparison %>% 
 

# obtain number of delta observations
n <- length(data_2011_to_present$delta) 
print(n)

formula_ci <- comparison %>% 
# YOUR CODE GOES HERE
  
  
# display the found CI  
formula_ci
```

> Explain the meaning of a confidence interval. 
Explain the effect of using a 90% Confidence vs a 95% confidence on the width of the constructed CI.


# All done!
